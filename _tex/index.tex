% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{agujournal2019}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{lineno}
\usepackage[inline]{trackchanges} %for better track changes. finalnew option will compile document with changes incorporated.
\usepackage{soul}
\linenumbers
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Revisiting Prompt Engineering: Strategic Interactions with Large Language Models for Application Development},
  pdfauthor={Michael Borck},
  pdfkeywords={Prompt Engineering, Large Language Models
(LLMs), Programming Paradigm, Application Development, Empirical
Techniques},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\journalname{BARG Curtin University}

\draftfalse

\begin{document}
\title{Revisiting Prompt Engineering: Strategic Interactions with Large
Language Models for Application Development}

\authors{Michael Borck\affil{1}}
\affiliation{1}{Business Information Systems, Curtin University, Perth
Australia, }
\correspondingauthor{Michael Borck}{michael.borck@curtin.edu.au}


\begin{abstract}
Prompt engineering has emerged as a crucial technique for leveraging the
capabilities of large language models (LLMs) in application development.
This paper explores the nuances of prompt engineering, presenting it not
merely as a tool for interaction but as a programming paradigm that
significantly eases the deployment of LLM capabilities into practical
applications. We examine various levels and strategies of prompt
engineering, highlighting their practical implications and potential for
simplifying complex tasks.
\end{abstract}

\section*{Plain Language Summary}
The paper discusses how prompt engineering can be used as a programming
paradigm to effectively integrate large language models (LLMs) like GPT
into software development. It describes two levels of prompt
engineering: a basic level for straightforward tasks such as email
drafting, and a more complex level involving programming interfaces for
deeper software integration, such as automatic grading systems. The
technique is portrayed as an empirical art that simplifies the
development process by minimising coding efforts and maximising AI
utility, although it faces challenges such as variability in
effectiveness and potential high costs. The paper advocates for more
research to enhance the scalability and efficiency of prompt
engineering.



\subsection{Introduction}\label{introduction}

The advent of large language models (LLMs) such as OpenAI's GPT series
has heralded a significant shift in the paradigms of programming and
computational linguistics. Traditionally, the interaction with these
models involved substantial coding efforts and a deep understanding of
machine learning algorithms. However, a more accessible approach has
surfaced, known as prompt engineering, which allows even those with
minimal coding expertise to leverage the advanced capabilities of LLMs
effectively. This paper delves into prompt engineering, defined as the
strategic formulation of input prompts to maximise the utility of LLMs
without internal model retraining. We argue that prompt engineering is
an underappreciated yet powerful tool for developers, capable of
achieving substantial outcomes with minimal effort.

Prompt engineering is defined as the strategic formulation of input
queries to maximise the utility and accuracy of responses from LLMs.
Initially perceived with skepticism, this methodology has quickly proven
its worth, demonstrating that a well-crafted prompt can extract
considerable value from these models with relatively low effort. The
essence of prompt engineering lies not in rewriting the underlying
algorithms of LLMs but in creatively interacting with them, using plain
language to ``program'' or guide the models towards desired outputs.

The primary objective of this paper is to systematically explore and
validate the effectiveness of various prompt engineering techniques. By
comparing the performance of ``good'' versus ``poor'' prompts, this
research aims to elucidate the impact of prompt construction on the
quality of responses generated by LLMs. We employ a series of metrics,
such as BERTscore and Perplexity, to quantitatively assess the
responses, providing a rigorous analysis of how different prompting
strategies affect the output of language models. This comparative study
is intended to offer valuable insights for both developers and
researchers, enabling them to harness the full potential of LLMs in
diverse applications ranging from automated content generation to
complex problem-solving tasks.

This paper includes embedded interactive elements that allow readers to
directly engage with and replicate our computational analyses. By
providing live data and code within the document, we aim to enhance
transparency and applicability of our findings. It allows readers to not
only digest the findings but also interact with the presented data and
analyses, potentially replicating or building upon the research in
real-time. This integration promises to bridge the gap between
theoretical exploration and practical application, making the research
accessible and applicable to a broad audience interested in the future
of artificial intelligence and machine learning.

\subsection{Prompt Engineering: A Dual-Level
Analysis}\label{prompt-engineering-a-dual-level-analysis}

Prompt engineering operates at two distinct levels. The first, an
intuitive and straightforward approach, utilises platforms like ChatGPT
to execute simple tasks (e.g., text summarisation, email drafting).
Although this method is accessible, its integration into larger systems
is non-trivial. The second level involves a more sophisticated
interaction through programming interfaces, such as Python APIs,
enabling the integration of LLM responses into broader software
ecosystems. This paper illustrates these levels with practical examples,
including the development of an LLM-powered automatic grading system for
educational applications.

\subsection{Methodology: Empirical Techniques in Prompt
Engineering}\label{methodology-empirical-techniques-in-prompt-engineering}

We categorise prompt engineering as an empirical art, where the
composition and formatting of prompts are tailored to enhance model
performance on specific tasks. This approach draws on the inherent
desire of language models to complete text-based tasks, effectively
``tricking'' them into performing complex functions beyond simple text
generation. We outline various techniques, such as the use of
descriptive language, structured prompts, and the chaining of thought
processes, to improve the interaction quality with LLMs.

This study systematically investigates the impact of prompt engineering
on the performance of large language models (LLMs). Our approach
involves comparing the efficacy of well-crafted (``good'') prompts
against less optimised (``poor'') prompts in eliciting accurate and
relevant responses from the models. We detail the experimental design,
tools utilised, and metrics applied for evaluating the results.

\paragraph{Experimental Design}\label{experimental-design}

The experiments are structured to assess two main types of prompts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Good Prompts}: These are designed with specific linguistic and
  structural techniques believed to enhance the LLM's understanding and
  response quality. Techniques include using descriptive language,
  structuring content, and employing contextual cues that guide the
  model towards the desired type of response.
\item
  \textbf{Poor Prompts}: These lack the strategic elements present in
  good prompts and typically include vague or ambiguous language that
  may lead to less accurate or relevant responses from the model.
\end{enumerate}

For each type of prompt, identical tasks are presented to the LLM to
ensure that the only variable affecting output quality is the prompt
construction itself. Tasks include generating text-based responses
across several domains, such as summarising passages, answering
questions, and creating content based on given specifications.

\paragraph{Tools and Models}\label{tools-and-models}

The study utilises OpenAI's GPT-3 model due to its widespread adoption
and robust performance across a variety of natural language processing
tasks. We interact with GPT-3 using the following tools:

\begin{itemize}
\item
  \textbf{LangChain}: A Python library designed to facilitate the
  development of applications on top of LLMs. LangChain allows for
  structured interaction with GPT-3, managing prompt templates, and
  integrating response parsing mechanisms.
\item
  \textbf{Quarto}: To document and share our findings, we use Quarto, an
  interactive computing framework that allows embedding live code,
  visualisations, and narrative text. This choice supports the dynamic
  presentation of our methods and results, enabling readers to engage
  directly with the analyses.
\end{itemize}

\paragraph{Metrics for Evaluation}\label{metrics-for-evaluation}

To objectively evaluate the quality of the responses generated by the
LLM under different prompting conditions, we employ several metrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{BERTscore}: This metric computes the semantic similarity
  between the generated text and a set of reference texts. It is used to
  assess how well the content produced by the LLM aligns with expected
  outputs in terms of meaning and context.
\item
  \textbf{Perplexity}: Typically used to measure how well a probability
  model predicts a sample, perplexity in our context helps determine how
  ``surprised'' the model is by the responses it generates, which
  indirectly indicates the naturalness and fluency of the text.
\item
  \textbf{Manual Review}: To complement automated metrics, responses are
  also subjectively evaluated by human reviewers for relevance,
  coherence, and informativeness. This step ensures that our findings
  account for qualitative aspects of text generation that automated
  metrics may overlook.
\end{enumerate}

\paragraph{Data Collection and
Analysis}\label{data-collection-and-analysis}

Responses from the LLM are collected under controlled conditions to
ensure consistency across tests. Each response is logged along with the
prompt used, allowing for detailed comparative analysis. The data are
analysed using statistical tools to identify significant differences in
performance metrics between good and poor prompts. Visualisations are
created to illustrate these differences clearly, providing both
quantitative and qualitative insights into the effectiveness of various
prompt engineering techniques.

\paragraph{Comparative Approach}\label{comparative-approach}

To evaluate the quality of prompts we treat the responses as references
for each other. This can be an effective way to highlight which prompt
elicits a more informative or relevant response on a given topic. Here's
how we implement this approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a topic or question you want to ask ChatGPT.
\item
  Craft two different prompts for the same topic - one that you consider
  a ``poor'' or suboptimal prompt, and one that you think is a ``good''
  or well-designed prompt.
\item
  Provide both prompts to ChatGPT and obtain the responses.
\item
  Treat the response from the ``good'' prompt as the reference, and
  evaluate the response from the ``poor'' prompt against it using
  metrics like:

  \begin{itemize}
  \tightlist
  \item
    BERTScore: Calculate semantic similarity between the two responses
    using contextual embeddings.{[}1{]}
  \item
    ROUGE/BLEU: Measure n-gram overlap between the responses.{[}2{]}
  \item
    Human evaluation: Have human raters judge which response is more
    informative, relevant, and coherent.
  \end{itemize}
\item
  Repeat the process, treating the response from the ``poor'' prompt as
  the reference, and evaluate the ``good'' prompt's response against it.
\end{enumerate}

By comparing the responses in this way, we identify which prompt leads
to a more desirable output. A higher BERTScore, ROUGE, or BLEU score
when using one response as the reference would indicate that the
corresponding prompt produced a more relevant and informative
response.{[}1{]}{[}2{]}{[}4{]}

Additionally, human evaluation can provide valuable qualitative insights
into the strengths and weaknesses of each prompt, complementing the
quantitative metrics.

This comparative approach leverages the fact that for the same topic, a
well-crafted prompt should elicit a more relevant and high-quality
response from the language model. By directly comparing the responses,
you can empirically evaluate the effectiveness of your prompt
engineering efforts.{[}1{]}{[}2{]}{[}4{]}{[}5{]}

Citations: {[}1{]} https://quaintitative.com/compare-llms/ {[}2{]}
https://andrewmaynard.net/comparative-prompts/ {[}3{]}
https://www.vcestudyguides.com/blog/the-five-types-of-text-response-prompts-archive
{[}4{]} https://agio.com/comparing-ai-prompting-strategies/ {[}5{]}
https://typeset.io/questions/how-do-different-types-of-prompts-affect-the-quality-of-2rmbjrcisy

Through this comprehensive methodology, the study aims to provide
actionable insights into the art of prompt engineering, guiding users on
how to best utilise LLMs for a range of applications.

Even without a predefined reference text, you can still compare the
responses from a good prompt and a poor prompt using metrics like
BERTScore or BLEU score. Here's how you can approach this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Obtain responses from the language model using both the good prompt
  and the poor prompt.
\item
  Treat the response from the good prompt as the ``reference'' text and
  the response from the poor prompt as the ``candidate'' text.
\item
  Calculate the BERTScore or BLEU score between the reference (good
  prompt response) and candidate (poor prompt response).
\item
  Repeat the process by treating the poor prompt response as the
  reference and the good prompt response as the candidate.
\item
  Compare the scores in both directions to see which prompt elicited a
  response that is more similar/dissimilar to the other.
\end{enumerate}

For BERTScore: - A higher BERTScore when using the good prompt response
as reference indicates the poor prompt response is more semantically
similar to the desired response.{[}1{]}{[}4{]} - You can calculate
BERTScore F1, precision, and recall to get a more nuanced comparison.

For BLEU score: - A higher BLEU score when using the good prompt
response as reference suggests the poor prompt response has more n-gram
overlap with the desired response.{[}5{]} - BLEU is based on precise
word matching, so it may not fully capture semantic similarities.

This comparative approach lets you evaluate prompt quality without an
external reference, by treating one prompt's response as the target
output.{[}1{]}{[}4{]}{[}5{]}

Additionally, you can complement the metrics with human evaluation by
having raters judge which response is more coherent, relevant and
informative for the given topic.{[}1{]}{[}4{]}

The key advantage is directly comparing the outputs from different
prompts to assess which one yields a more desirable response from the
language model.{[}1{]}{[}4{]}{[}5{]}

Citations: {[}1{]} https://arxiv.org/pdf/2305.12421.pdf {[}2{]}
https://www.vcestudyguides.com/blog/the-five-types-of-text-response-prompts-archive
{[}3{]} https://quaintitative.com/compare-llms/ {[}4{]}
https://aclanthology.org/2021.wmt-1.59.pdf {[}5{]}
https://aclanthology.org/P02-1040.pdf

\subsection{Results}\label{results}

This section presents the comprehensive results of our investigation
into the effectiveness of prompt engineering techniques for enhancing
the performance of large language models (LLMs). Our analysis focuses on
quantitatively evaluating the impact of well-constructed versus poorly
constructed prompts on the quality of the responses generated by LLMs.
We utilise three established metrics for this evaluation: BLEU Score,
ROUGE Score, and BERTscore, each providing insights into different
aspects of text quality such as precision, recall, and semantic
similarity. The results clearly demonstrate the significant influence
that prompt construction can have on the accuracy, relevance, and
fluency of the generated text. Additionally, we provide detailed,
interactive examples that not only illustrate these effects but also
allow readers to explore the nuances of prompt engineering through live
code. These examples showcase practical applications and underscore the
practical implications of our findings, bridging theoretical research
with actionable insights. Through dynamic visualisations and interactive
Quarto cells, readers are invited to engage directly with the data,
enhancing their understanding of how strategic prompt design can be
effectively utilised in real-world applications.

\subsubsection{Overview of Experimental
Results}\label{overview-of-experimental-results}

\begin{itemize}
\tightlist
\item
  Begin with a summary of the findings from the comparative analysis of
  good vs.~poor prompts. This would involve presenting quantitative data
  from BERTscore, Perplexity, and manual reviews.
\item
  Include charts, graphs, or tables that clearly depict the differences
  in LLM performance based on the type of prompt used.
\item
  Compare the outputs and effectiveness of the LLM across different
  prompting techniques illustrated in the detailed examples and the case
  study. This section would analyse the broader implications of prompt
  engineering on practical applications.
\item
  Discuss how variations in prompt construction can lead to significant
  differences in output quality and application functionality.
\end{itemize}

\subsubsection{Detailed Examples of Good vs.~Poor
Prompts}\label{detailed-examples-of-good-vs.-poor-prompts}

This section provides detailed descriptions and Python code examples for
various prompt engineering tricks used to enhance the performance of
large language models (LLMs). Each strategy is designed to optimise the
interaction with LLMs in different contexts, demonstrating practical
applications and potential benefits.

\paragraph{Strategy 1: Be Descriptive (More is
Better)}\label{strategy-1-be-descriptive-more-is-better}

\textbf{Explanation}: Providing detailed information within prompts
helps the LLM understand the context better, leading to more accurate
and relevant responses. This strategy is particularly useful in
scenarios where the details are critical to the task, such as generating
personalised content or specific instructions.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Importing the OpenAI GPT library}
\ImportTok{from}\NormalTok{ openai }\ImportTok{import}\NormalTok{ ChatCompletion}

\CommentTok{\# Initialise the model with your API key}
\NormalTok{openai\_api\_key }\OperatorTok{=} \StringTok{\textquotesingle{}your{-}api{-}key\textquotesingle{}}
\NormalTok{chat\_model }\OperatorTok{=}\NormalTok{ ChatCompletion(api\_key}\OperatorTok{=}\NormalTok{openai\_api\_key, model}\OperatorTok{=}\StringTok{"gpt{-}3.5{-}turbo"}\NormalTok{)}

\CommentTok{\# Define a detailed prompt for a birthday message}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{Write a birthday message for my dad. He is turning 60 years old, loves golf, enjoys classical music, and appreciates good humor. The message should be heartfelt and include a joke about getting older.}
\StringTok{"""}

\CommentTok{\# Generate the message using the detailed prompt}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\paragraph{Strategy 2: Give Examples (Few-Shot
Learning)}\label{strategy-2-give-examples-few-shot-learning}

\textbf{Explanation}: Demonstrating desired outputs through examples can
significantly improve model performance, especially in tasks requiring
specific formats or styles. This strategy is beneficial in educational
settings, content creation, or anywhere model guidance through exemplars
can enhance output relevancy.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a prompt with examples for writing subtitles}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{Given the title of a blog post, write a subtitle that captures the essence of the article.}

\StringTok{Title: Advances in Renewable Energy}
\StringTok{Subtitle: Exploring the latest breakthroughs in sustainable power sources}

\StringTok{Title: The Future of AI in Medicine}
\StringTok{Subtitle: How AI is revolutionising diagnostics and patient care}

\StringTok{Title: Innovations in Educational Technology}
\StringTok{Subtitle: New tools that are transforming how students learn}

\StringTok{Title: The Role of Genetics in Public Health}
\StringTok{Subtitle: }
\StringTok{"""}

\CommentTok{\# Generate a subtitle for a new article}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\paragraph{Strategy 3: Use Structured
Text}\label{strategy-3-use-structured-text}

\textbf{Explanation}: Structuring prompts in a clear and organised
manner can help the model parse and process the information more
effectively. This approach is ideal for tasks that require data
extraction, summarisation, or any application where clarity and
precision are crucial.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a structured prompt for a recipe}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{Create a recipe for a chocolate cake. The recipe should include:}
\StringTok{{-} Title: Simple Chocolate Cake}
\StringTok{{-} Ingredients: List all ingredients with precise measurements}
\StringTok{{-} Instructions: Provide a step{-}by{-}step guide on how to prepare the cake}
\StringTok{"""}

\CommentTok{\# Generate the structured recipe}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\paragraph{Strategy 4: Chain of
Thought}\label{strategy-4-chain-of-thought}

\textbf{Explanation}: Encouraging the model to ``think'' step-by-step
can improve its reasoning abilities, particularly useful in
problem-solving or tasks requiring logical progression, such as
technical troubleshooting or complex decision-making.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a prompt that uses the chain of thought approach}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{Problem: A user is unable to access their email account after multiple password resets. Let\textquotesingle{}s think step by step to diagnose the issue:}

\StringTok{Step 1: Verify if the user is using the correct email address.}
\StringTok{Step 2: Check if their account is locked due to too many failed attempts.}
\StringTok{Step 3: Ensure the password reset process is completed correctly.}
\StringTok{Step 4: Suggest recovery through an alternate email or security questions.}

\StringTok{What could be the issue based on these steps?}
\StringTok{"""}

\CommentTok{\# Generate a diagnostic response using the chain of thought}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\paragraph{Strategy 5: Chatbot
Personas}\label{strategy-5-chatbot-personas}

\textbf{Explanation}: Prompting an LLM to adopt a specific persona can
tailor its responses to fit the desired character or expertise level.
This strategy is particularly useful in customer service bots,
educational tools, or any application where engaging and role-specific
dialogue is beneficial.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Importing the OpenAI GPT library}
\ImportTok{from}\NormalTok{ openai }\ImportTok{import}\NormalTok{ ChatCompletion}

\CommentTok{\# Initialise the model with your API key}
\NormalTok{openai\_api\_key }\OperatorTok{=} \StringTok{\textquotesingle{}your{-}api{-}key\textquotesingle{}}
\NormalTok{chat\_model }\OperatorTok{=}\NormalTok{ ChatCompletion(api\_key}\OperatorTok{=}\NormalTok{openai\_api\_key, model}\OperatorTok{=}\StringTok{"gpt{-}3.5{-}turbo"}\NormalTok{)}

\CommentTok{\# Define a prompt where the model adopts a persona}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{You are an expert gardener. A novice gardener asks for advice on starting a vegetable garden in a small backyard. Provide a friendly and detailed response that includes tips on soil preparation, choosing vegetables, and ongoing care.}
\StringTok{"""}

\CommentTok{\# Generate a response using the gardener persona}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\paragraph{Strategy 6: Flipped
Approach}\label{strategy-6-flipped-approach}

\textbf{Explanation}: This technique involves prompting the LLM to ask
questions back to the user to gain a better understanding of their
needs. It's especially effective in consultation services or any
scenario where clarifying user intent is crucial for accurate responses.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a prompt that uses the flipped approach for better understanding}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{You are a travel consultant. A client is planning a trip to Europe but hasn\textquotesingle{}t provided much detail. Ask the client a series of questions to determine their preferences for destinations, travel style, budget, and any special interests they might have.}
\StringTok{"""}

\CommentTok{\# Generate a series of questions to help tailor the travel advice}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\paragraph{Strategy 7: Reflect, Review, and
Refine}\label{strategy-7-reflect-review-and-refine}

\textbf{Explanation}: This strategy involves prompting the LLM to
evaluate and potentially revise its previous responses. It is ideal for
iterative tasks such as editing, programming, or any creative process
where progressive refinement enhances the final output.

\textbf{Python Code Example}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a prompt that encourages reflection and refinement of a response}
\NormalTok{prompt }\OperatorTok{=} \StringTok{"""}
\StringTok{Initially, you wrote a brief summary of the novel \textquotesingle{}To Kill a Mockingbird\textquotesingle{}. Now, review your summary, identify any inaccuracies or areas lacking in detail, and provide an improved version.}
\StringTok{"""}

\CommentTok{\# Generate an improved summary through self{-}review and refinement}
\NormalTok{response }\OperatorTok{=}\NormalTok{ chat\_model.create(}
\NormalTok{    messages}\OperatorTok{=}\NormalTok{[\{}\StringTok{"role"}\NormalTok{: }\StringTok{"system"}\NormalTok{, }\StringTok{"content"}\NormalTok{: prompt\}]}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(response[}\StringTok{\textquotesingle{}choices\textquotesingle{}}\NormalTok{][}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}message\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}content\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Each of these tricks serves to enhance the interactivity and
effectiveness of LLMs in specific scenarios. By employing these methods,
developers can tailor the behavior of LLMs to meet diverse requirements
and improve user experience, demonstrating the flexibility and potential
of prompt engineering in practical applications. These examples
illustrate how different prompt engineering tricks can be tailored to
enhance LLM performance across a variety of tasks. The context in which
each strategy is applied highlights its potential to improve the
relevance and accuracy of the model's responses, making them invaluable
tools in the arsenal of developers and researchers working with LLMs.

\subsubsection{Comparative Analysis}\label{comparative-analysis}

\paragraph{BLUE Score}\label{blue-score}

Here's an example code to compare the responses from a good prompt and a
poor prompt using the BLEU score in Python:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ nltk.translate.bleu\_score }\ImportTok{import}\NormalTok{ sentence\_bleu}
\ImportTok{import}\NormalTok{ nltk}

\CommentTok{\# Download required NLTK data}
\NormalTok{nltk.download(}\StringTok{\textquotesingle{}punkt\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Define the good and poor prompts}
\NormalTok{good\_prompt }\OperatorTok{=} \StringTok{"Explain the theory of relativity in simple terms."}
\NormalTok{poor\_prompt }\OperatorTok{=} \StringTok{"What is relativity?"}

\CommentTok{\# Get responses from the language model (replace with your own code)}
\NormalTok{good\_response }\OperatorTok{=} \StringTok{"The theory of relativity, proposed by Albert Einstein, revolves around two main ideas: special relativity and general relativity. Special relativity states that the laws of physics are the same for all non{-}accelerating observers, and that the speed of light in a vacuum is independent of the motion of all observers. General relativity describes gravity not as a force, but as a consequence of the curvature of spacetime caused by the presence of mass and energy."}
\NormalTok{poor\_response }\OperatorTok{=} \StringTok{"Relativity is a scientific theory developed by Albert Einstein."}

\CommentTok{\# Tokenise the responses}
\NormalTok{good\_response\_tokens }\OperatorTok{=}\NormalTok{ nltk.word\_tokenise(good\_response)}
\NormalTok{poor\_response\_tokens }\OperatorTok{=}\NormalTok{ nltk.word\_tokenise(poor\_response)}

\CommentTok{\# Calculate BLEU scores}
\NormalTok{bleu\_score\_good\_as\_ref }\OperatorTok{=}\NormalTok{ sentence\_bleu([good\_response\_tokens], poor\_response\_tokens)}
\NormalTok{bleu\_score\_poor\_as\_ref }\OperatorTok{=}\NormalTok{ sentence\_bleu([poor\_response\_tokens], good\_response\_tokens)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"BLEU score (good response as reference): }\SpecialCharTok{\{}\NormalTok{bleu\_score\_good\_as\_ref}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"BLEU score (poor response as reference): }\SpecialCharTok{\{}\NormalTok{bleu\_score\_poor\_as\_ref}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here's how the code works:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We import the necessary modules and download the required NLTK data
  for tokenisation.
\item
  We define the good and poor prompts as strings.
\item
  We obtain the responses from the language model for the good and poor
  prompts. In this example, we've hardcoded the responses, but in
  practice, you would replace these with your own code to generate
  responses from the language model.
\item
  We tokenise the good and poor responses using
  \texttt{nltk.word\_tokenise}.
\item
  We calculate the BLEU scores in both directions:

  \begin{itemize}
  \tightlist
  \item
    \texttt{bleu\_score\_good\_as\_ref} treats the good prompt response
    as the reference and the poor prompt response as the candidate.
  \item
    \texttt{bleu\_score\_poor\_as\_ref} treats the poor prompt response
    as the reference and the good prompt response as the candidate.
  \end{itemize}
\item
  We print out both BLEU scores.
\end{enumerate}

When you run this code, you should see output similar to:

\begin{verbatim}
BLEU score (good response as reference): 0.235
BLEU score (poor response as reference): 0.087
\end{verbatim}

In this example, the BLEU score is higher when using the good prompt
response as the reference (0.235) compared to using the poor prompt
response as the reference (0.087). This suggests that the response from
the good prompt has more n-gram overlap with the response from the poor
prompt, indicating that the good prompt elicited a more informative and
relevant response.

You can interpret the BLEU scores as follows:

\begin{itemize}
\tightlist
\item
  A higher BLEU score when using the good prompt response as the
  reference suggests that the poor prompt response is more similar to
  the desired response (from the good prompt).
\item
  A lower BLEU score when using the poor prompt response as the
  reference indicates that the good prompt response is less similar to
  the undesirable response (from the poor prompt).
\end{itemize}

By comparing the BLEU scores in both directions, you can evaluate which
prompt yielded a response that is more relevant and informative for the
given topic.

\paragraph{ROUGE (Recall-Oriented Understudy for Gisting
Evaluation)}\label{rouge-recall-oriented-understudy-for-gisting-evaluation}

The ROGUE score is a set of metrics used primarily for evaluating
automatic summarisation and machine translation. It measures the overlap
between the system-generated text and reference texts, focusing on the
number of overlapping units such as n-grams, word sequences, and word
pairs. The most commonly used variants are ROUGE-N (which measures
n-gram overlap), ROUGE-L (which measures the longest common
subsequence), and ROUGE-S (which measures skip-bigram overlap). Below,
we implement the ROUGE-L score in Python to evaluate the quality of
responses from language models. This will require the installation of
the \texttt{rouge} package, which can be done using pip:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pip}\NormalTok{ install rouge}
\end{Highlighting}
\end{Shaded}

\subsubsection{Python Implementation Example for
ROUGE-L}\label{python-implementation-example-for-rouge-l}

Here's an example of how you can calculate the ROUGE-L score for a
language model's responses using the Python \texttt{rouge} library:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ rouge }\ImportTok{import}\NormalTok{ Rouge}

\CommentTok{\# Initialise the ROUGE scorer}
\NormalTok{rouge }\OperatorTok{=}\NormalTok{ Rouge()}

\CommentTok{\# Example reference and system{-}generated summaries}
\NormalTok{reference }\OperatorTok{=} \StringTok{"John F. Kennedy was the 35th president of the United States. He served from 1961 until his assassination in 1963."}
\NormalTok{system\_generated\_1 }\OperatorTok{=} \StringTok{"JFK, also known as John Kennedy, was the president of the US who served from 1961 to 1963."}
\NormalTok{system\_generated\_2 }\OperatorTok{=} \StringTok{"The 35th president of the United States was John F. Kennedy, who served from January 1961 until his assassination in November 1963."}

\CommentTok{\# Calculate ROUGE scores}
\NormalTok{scores\_1 }\OperatorTok{=}\NormalTok{ rouge.get\_scores(system\_generated\_1, reference)}
\NormalTok{scores\_2 }\OperatorTok{=}\NormalTok{ rouge.get\_scores(system\_generated\_2, reference)}

\CommentTok{\# Print ROUGE{-}L scores for each system{-}generated summary}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ROUGE{-}L Score for System{-}Generated Summary 1:"}\NormalTok{, scores\_1[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}rouge{-}l\textquotesingle{}}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ROUGE{-}L Score for System{-}Generated Summary 2:"}\NormalTok{, scores\_2[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}rouge{-}l\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsubsection{Explanation}\label{explanation}

\begin{itemize}
\item
  \textbf{ROUGE-L}: This score focuses on the longest common subsequence
  between the system-generated text and the reference text. It considers
  sentence-level structure similarity naturally and identifies longest
  co-occurring in-sequence n-grams of words. It provides two scores:
  precision (what fraction of the generated words are relevant) and
  recall (what fraction of the reference words were captured by the
  generated text), and an F-measure which is the harmonic mean of
  precision and recall.
\item
  \textbf{System-Generated Summaries}: These are the texts generated by
  your system, in this case, responses from a language model.
\item
  \textbf{Reference Summary}: This is the ``ideal'' or target summary
  against which the system-generated summaries are compared.
\end{itemize}

\subsubsection{Usage Considerations}\label{usage-considerations}

\begin{itemize}
\tightlist
\item
  \textbf{Contextual Relevance}: While ROUGE scores can provide
  objective measures of textual overlap, they might not fully account
  for the semantic accuracy or contextual relevance of the generated
  text. For more nuanced evaluations, consider combining ROUGE with
  manual qualitative assessments.
\item
  \textbf{Variants}: Depending on the specific requirements of your
  evaluation, you might choose different ROUGE metrics (like ROUGE-N for
  n-gram overlap, or ROUGE-S for skip-bigrams) to focus on different
  aspects of the comparison.
\end{itemize}

This approach allows you to quantitatively assess how well a language
model's responses match a reference, providing a standardised measure to
compare different prompting strategies or model configurations in your
research or application development.

\subsubsection{Using BERTscore without a Predefined
Reference}\label{using-bertscore-without-a-predefined-reference}

Using BERTscore to compare the responses to a ``poor'' prompt and a
``good'' prompt is a viable approach, especially when you are trying to
assess how effectively each prompt elicits the intended response from a
language model. Essentially, you would compare the response to each type
of prompt against a reference standard or even against each other to
determine which prompt results in a more semantically rich and relevant
response.

If you don't have a predefined reference response but want to compare
the quality of responses between two prompts, you can consider the
following approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Use Responses as Mutual References}: Treat the response from
  one prompt as the reference for the other and vice versa. This
  comparative approach can highlight which prompt elicits a response
  that is more informative or relevant to the topic.
\item
  \textbf{Create a Synthetic Reference}: If you know what information
  the response should contain, you can construct a synthetic reference
  response that includes all the expected elements. This reference can
  then be used to evaluate the responses from both the poor and good
  prompts.
\end{enumerate}

Here's how to implement the first approach using the BERTscore:

\paragraph{Implementation of BERTscore
Comparison}\label{implementation-of-bertscore-comparison}

First, ensure you have the \texttt{bert-score} package installed:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pip}\NormalTok{ install bert{-}score}
\end{Highlighting}
\end{Shaded}

Then, set up your comparison:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ bert\_score }\ImportTok{import}\NormalTok{ score}

\CommentTok{\# Assuming you\textquotesingle{}ve obtained responses from the model}
\NormalTok{response\_from\_poor\_prompt }\OperatorTok{=} \StringTok{"The quick brown fox tries to jump but fails."}
\NormalTok{response\_from\_good\_prompt }\OperatorTok{=} \StringTok{"The quick brown fox jumps over the lasy dog."}

\CommentTok{\# Using each other as references}
\NormalTok{P\_poor, R\_poor, F1\_poor }\OperatorTok{=}\NormalTok{ score([response\_from\_poor\_prompt], [response\_from\_good\_prompt], lang}\OperatorTok{=}\StringTok{"en"}\NormalTok{)}
\NormalTok{P\_good, R\_good, F1\_good }\OperatorTok{=}\NormalTok{ score([response\_from\_good\_prompt], [response\_from\_poor\_prompt], lang}\OperatorTok{=}\StringTok{"en"}\NormalTok{)}

\CommentTok{\# Print results}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Comparison using Good Prompt as Reference:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Precision: }\SpecialCharTok{\{}\NormalTok{P\_poor}\SpecialCharTok{.}\NormalTok{tolist()}\SpecialCharTok{\}}\SpecialStringTok{, Recall: }\SpecialCharTok{\{}\NormalTok{R\_poor}\SpecialCharTok{.}\NormalTok{tolist()}\SpecialCharTok{\}}\SpecialStringTok{, F1 Score: }\SpecialCharTok{\{}\NormalTok{F1\_poor}\SpecialCharTok{.}\NormalTok{tolist()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Comparison using Poor Prompt as Reference:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Precision: }\SpecialCharTok{\{}\NormalTok{P\_good}\SpecialCharTok{.}\NormalTok{tolist()}\SpecialCharTok{\}}\SpecialStringTok{, Recall: }\SpecialCharTok{\{}\NormalTok{R\_good}\SpecialCharTok{.}\NormalTok{tolist()}\SpecialCharTok{\}}\SpecialStringTok{, F1 Score: }\SpecialCharTok{\{}\NormalTok{F1\_good}\SpecialCharTok{.}\NormalTok{tolist()}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Interpretation}\label{interpretation}

\begin{itemize}
\tightlist
\item
  \textbf{Precision}: Measures how much of the information in the
  generated response is relevant (i.e., how much of the generated
  content actually pertains to what the reference response discusses).
\item
  \textbf{Recall}: Measures how much of the reference's content is
  covered by the generated response (i.e., how much of the essential
  information in the reference was captured in the generated response).
\item
  \textbf{F1 Score}: The harmonic mean of precision and recall,
  providing a single score that balances both completeness and accuracy.
\end{itemize}

\subsubsection{Considerations}\label{considerations}

This method of using responses as mutual references can be particularly
useful when direct comparison metrics are needed to evaluate the
effectiveness of different prompts in eliciting detailed and relevant
responses. It is somewhat subjective, as it assumes that one of the
responses contains sufficient quality content to serve as a
benchmark---a fair assumption if one prompt is indeed superior.

By comparing these metrics, you can objectively analyse which prompt
results in a better response in terms of information richness and
relevance, thereby supporting the effectiveness of your prompt
engineering techniques.

\subsection{Case Study: Building an Automatic
Grader}\label{case-study-building-an-automatic-grader}

The practical application of prompt engineering is demonstrated through
the development of an automatic grader for educational purposes. Using
LangChain, a Python library for LLM integration, we construct a
prompt-based system that evaluates student responses to open-ended
questions. This system exemplifies the shift from traditional
programming methods to a model where significant portions of logic are
outsourced to an LLM, highlighting the efficiency and scalability of
prompt engineering.

This section provides a detailed guide to building an automatic grader
application utilising LangChain, a Python library designed to simplify
the integration of large language models (LLMs) in application
development. The application leverages prompt engineering to evaluate
student responses in a high school history class, demonstrating the
practical implementation of LLMs in educational settings.

\paragraph{Overview}\label{overview}

The automatic grader application is designed to assess short-answer
questions, where answers might vary but still be correct. The grader
needs to handle different phrasings, synonyms, and minor spelling errors
effectively.

\paragraph{Setup and Dependencies}\label{setup-and-dependencies}

Before starting, ensure you have Python installed along with the
necessary libraries. You'll need \texttt{langchain} and \texttt{openai}.
If not already installed, they can be added via pip:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pip}\NormalTok{ install langchain openai}
\end{Highlighting}
\end{Shaded}

\paragraph{Code Implementation}\label{code-implementation}

\textbf{Step 1: Import Necessary Libraries}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import necessary modules from LangChain and OpenAI}
\ImportTok{from}\NormalTok{ langchain.chat\_models }\ImportTok{import}\NormalTok{ ChatOpenAI}
\ImportTok{from}\NormalTok{ langchain.prompts }\ImportTok{import}\NormalTok{ PromptTemplate}
\ImportTok{from}\NormalTok{ langchain.chains }\ImportTok{import}\NormalTok{ LLMChain}
\ImportTok{from}\NormalTok{ langchain.schema }\ImportTok{import}\NormalTok{ BaseOutputParser}
\end{Highlighting}
\end{Shaded}

\textbf{Step 2: Define the Prompt Template} The prompt template mimics a
high school history teacher grading an assignment. It should account for
the question, the correct answer, and the student's response.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define the prompt template for grading}
\NormalTok{prompt\_template\_text }\OperatorTok{=} \StringTok{"""}
\StringTok{You are a high school history teacher grading homework assignments. }
\StringTok{Based on the homework question indicated by "**Q:**" and the correct answer }
\StringTok{indicated by "**A:**", your task is to determine whether the student\textquotesingle{}s answer is correct. }
\StringTok{Grading is binary; therefore, student answers can be correct or wrong. }
\StringTok{Simple misspellings are okay.}

\StringTok{**Q:** }\SpecialCharTok{\{question\}}
\StringTok{**A:** }\SpecialCharTok{\{correct\_answer\}}

\StringTok{**Student\textquotesingle{}s Answer:** }\SpecialCharTok{\{student\_answer\}}
\StringTok{"""}
\end{Highlighting}
\end{Shaded}

\textbf{Step 3: Initialise LangChain}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define and initialise the ChatOpenAI model with your OpenAI API key}
\NormalTok{chat\_model }\OperatorTok{=}\NormalTok{ ChatOpenAI(openai\_api\_key}\OperatorTok{=}\StringTok{"your{-}openai{-}api{-}key"}\NormalTok{, temperature}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\CommentTok{\# Create a prompt template object}
\NormalTok{prompt }\OperatorTok{=}\NormalTok{ PromptTemplate(}
\NormalTok{    input\_variables}\OperatorTok{=}\NormalTok{[}\StringTok{"question"}\NormalTok{, }\StringTok{"correct\_answer"}\NormalTok{, }\StringTok{"student\_answer"}\NormalTok{],}
\NormalTok{    template}\OperatorTok{=}\NormalTok{prompt\_template\_text}
\NormalTok{)}

\CommentTok{\# Define the chain using the LLM and the prompt}
\NormalTok{chain }\OperatorTok{=}\NormalTok{ LLMChain(llm}\OperatorTok{=}\NormalTok{chat\_model, prompt}\OperatorTok{=}\NormalTok{prompt)}
\end{Highlighting}
\end{Shaded}

\textbf{Step 4: Output Parser} This component converts the LLM's
response into a structured format that can be easily interpreted by
other systems.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define an output parser to interpret the LLM\textquotesingle{}s grading response}
\KeywordTok{class}\NormalTok{ GradeOutputParser(BaseOutputParser):}
    \CommentTok{"""Parses the LLM\textquotesingle{}s response to determine if the answer is correct or wrong."""}
    
    \KeywordTok{def}\NormalTok{ parse(}\VariableTok{self}\NormalTok{, text: }\BuiltInTok{str}\NormalTok{):}
        \CommentTok{"""Check if the response indicates the student\textquotesingle{}s answer was wrong."""}
        \ControlFlowTok{return} \StringTok{"wrong"} \KeywordTok{not} \KeywordTok{in}\NormalTok{ text.lower()}

\CommentTok{\# Update the chain to use the new output parser}
\NormalTok{chain.output\_parser }\OperatorTok{=}\NormalTok{ GradeOutputParser()}
\end{Highlighting}
\end{Shaded}

\textbf{Step 5: Running the Grader}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample question and answers}
\NormalTok{question }\OperatorTok{=} \StringTok{"Who was the 35th president of the United States of America?"}
\NormalTok{correct\_answer }\OperatorTok{=} \StringTok{"John F. Kennedy"}
\NormalTok{student\_answers }\OperatorTok{=}\NormalTok{ [}\StringTok{"JFK"}\NormalTok{, }\StringTok{"John F Kennedy"}\NormalTok{, }\StringTok{"John Kennedy"}\NormalTok{, }\StringTok{"Jack Kennedy"}\NormalTok{]}

\CommentTok{\# Evaluate each student answer}
\ControlFlowTok{for}\NormalTok{ student\_answer }\KeywordTok{in}\NormalTok{ student\_answers:}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ chain.run(\{}\StringTok{\textquotesingle{}question\textquotesingle{}}\NormalTok{: question, }\StringTok{\textquotesingle{}correct\_answer\textquotesingle{}}\NormalTok{: correct\_answer, }\StringTok{\textquotesingle{}student\_answer\textquotesingle{}}\NormalTok{: student\_answer\})}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Question: }\SpecialCharTok{\{}\NormalTok{question}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Student Answer: }\SpecialCharTok{\{}\NormalTok{student\_answer}\SpecialCharTok{\}}\SpecialStringTok{ {-} }\SpecialCharTok{\{}\StringTok{\textquotesingle{}Correct\textquotesingle{}} \ControlFlowTok{if}\NormalTok{ result }\ControlFlowTok{else} \StringTok{\textquotesingle{}Incorrect\textquotesingle{}}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This example application showcases how LangChain can be utilised to
develop practical, LLM-integrated solutions for real-world problems such
as grading. The automatic grader is not only efficient but also
demonstrates the capability of LLMs to handle variability in natural
language processing tasks, making it an ideal solution for educational
applications.

\subsection{Discussion: Efficacy and Limitations of Prompt
Engineering}\label{discussion-efficacy-and-limitations-of-prompt-engineering}

While prompt engineering offers substantial advantages, including
significant reductions in development time and increased flexibility, it
is not without limitations. The effectiveness of prompt strategies can
vary significantly between different LLM versions, and the approach may
incur considerable computational costs. Furthermore, the general-purpose
nature of large models like ChatGPT may not be optimal for specialised
tasks, which could be better served by fine-tuning specific models
tailored to particular needs.

\subsection{Conclusion}\label{conclusion}

Prompt engineering represents a transformative approach to programming,
enabling developers to harness the capabilities of LLMs in a
user-friendly and cost-effective manner. As this field evolves, it is
expected that the techniques and strategies of prompt engineering will
become increasingly refined, paving the way for more sophisticated and
integrated applications across various domains.

\subsection{Future Work}\label{future-work}

Further research is needed to explore the scalability of prompt
engineering across different platforms and its integration with advanced
model fine-tuning techniques. Additionally, comparative studies could
elucidate the conditions under which prompt engineering is most
effective compared to traditional programming approaches.

\subsubsection{References}\label{references}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Talebi, S. (2023). Prompt Engineering: How to strategy AI into Solving
  Your Problems. \emph{Towards Data Science}.
\item
  Karpathy, A. (n.d.). \emph{Language Models as Few-Shot Learners}.
\item
  Wei, J., et al.~(2021). \emph{Chain of Thought Prompting Elicits
  Reasoning in Large Language Models}.
\end{enumerate}



\end{document}
